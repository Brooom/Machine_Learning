{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11b4cc74",
   "metadata": {},
   "source": [
    "# Home Assignment No. 1: Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda88da5",
   "metadata": {},
   "source": [
    "In this part of the homework, you are to solve several theoretical problems related to machine learning algorithms.\n",
    "\n",
    "* For every separate problem you can get **INTERMEDIATE scores**.\n",
    "\n",
    "\n",
    "* Your solution must me **COMPLETE**, i.e. contain all required formulas/proofs/detailed explanations.\n",
    "\n",
    "\n",
    "* You must write your solution for each problem right after the words **YOUR SOLUTION**. Attaching pictures of your handwriting is allowed, but **highly discouraged**.\n",
    "\n",
    "## $\\LaTeX$ in Jupyter\n",
    "\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "\n",
    "* to write **cases of equations** use \n",
    "```markdown\n",
    "$$ left-hand-side = \\begin{cases}\n",
    "                     right-hand-side on line 1, & \\text{condition} \\\\\n",
    "                     right-hand-side on line 2, & \\text{condition} \\\\\n",
    "                    \\end{cases} $$\n",
    "```\n",
    "\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "$$ \\begin{align}\n",
    "    left-hand-side on line 1 &= right-hand-side on line 1 \\\\\n",
    "    left-hand-side on line 2 &= right-hand-side on line 2\n",
    "   \\end{align} $$\n",
    "```\n",
    "\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518781b8-6d90-494c-99dd-ddd5ffaa90a8",
   "metadata": {},
   "source": [
    "## Task 1. Locally Weighted Linear Regression [6 points]\n",
    "\n",
    "Under the assumption that $\\mathbf{X}^\\top W(\\mathbf{x}_0) \\mathbf{X}$ is inverible, derive the closed form solution for the LWR problem, defined in Task 3 of the practical part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d699613-ed8c-4304-a85f-b2b9a7735a47",
   "metadata": {},
   "source": [
    "### Your solution:\n",
    "$$\n",
    "\\theta^*(\\mathbf{x}_0) = \\arg \\min_{\\theta(\\mathbf{x}_0)} \\sum_{i = 1}^m w^{(i)}(\\mathbf{x}_0) \\left(y_i - \\theta(\\mathbf{x}_0)^\\top \\mathbf{x}_i\\right)^2\n",
    "$$\n",
    "\n",
    "To minimize this, we calculate the gradient of the loss function with respect to $\\theta(\\mathbf{x}_0)$ and set it to zero.\n",
    "\n",
    "We have the following loss function:\n",
    "$$\n",
    "L(\\theta(\\mathbf{x}_0)) = \\sum_{i = 1}^m w^{(i)}(\\mathbf{x}_0) \\left(y_i - \\theta(\\mathbf{x}_0)^\\top \\mathbf{x}_i\\right)^2 = (\\mathbf{X}\\theta(\\mathbf{x}_0) - y) \\cdot W(\\mathbf{x}_0) \\cdot (\\mathbf{X}\\theta(\\mathbf{x}_0) - y)\n",
    "$$\n",
    "\n",
    "For simlicity we will denote $W(\\mathbf{x}_0) = W$ and $\\theta = \\theta(\\mathbf{x}_0)$.\n",
    "\n",
    "Now we will calculate the gradient of the loss function with respect to $\\theta$:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta L(\\theta) = \\nabla_\\theta \\left((\\mathbf{X}\\theta - y) \\cdot W \\cdot (\\mathbf{X}\\theta - y)\\right) = \\nabla_\\theta \\left(\\theta^\\top \\mathbf{X}^\\top W \\mathbf{X} \\theta - 2y^\\top W \\mathbf{X} \\theta + y^\\top W y\\right) \n",
    "= 2\\mathbf{X}^\\top W \\mathbf{X} \\theta - 2\\mathbf{X}^\\top W y = 0\n",
    "$$\n",
    "\n",
    "Now we can solve this equation for $\\theta$:\n",
    "\n",
    "$$\n",
    "\\mathbf{X}^\\top W \\mathbf{X} \\theta = \\mathbf{X}^\\top W y\n",
    "$$\n",
    "\n",
    "Because $\\mathbf{X}^\\top W \\mathbf{X}$ is invertible, we can multiply both sides by its inverse:\n",
    "\n",
    "$$\n",
    "\\theta = (\\mathbf{X}^\\top W \\mathbf{X})^{-1} \\mathbf{X}^\\top W y\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta(\\mathbf{x}_0) = (\\mathbf{X}^\\top W(\\mathbf{x}_0) \\mathbf{X})^{-1} \\mathbf{X}^\\top W(\\mathbf{x}_0) y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff2a8c7",
   "metadata": {},
   "source": [
    "## Task 2. Multiclass Naive Bayes Classifier [4 points]\n",
    "\n",
    "Let us consider **multiclass classification problem** with classes $C_1, \\dots, C_K$.\n",
    "\n",
    "Assume that all $d$ features $\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_d \\end{bmatrix}$ are **binary**, i.e. $x_{i} \\in \\{0, 1\\}$ for $i = \\overline{1, d}$ **or** feature vector $\\mathbf{x} \\in \\{0, 1\\}^d$.\n",
    "\n",
    "Show that the decision rule of a **Naive Bayes Classifier** can be represented as $\\arg\\max$ of linear functions of the input.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**Hint**: use the **maximum a posteriori** (MAP) decision rule: $\\hat{y} = \\arg\\max\\limits_{y \\in \\overline{1, K}} p(y)p(\\mathbf{x}|y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3372e710",
   "metadata": {},
   "source": [
    "### Your solution:\n",
    "\n",
    "The Naive Bayes Classifier uses the following decision rule:\n",
    "\n",
    "$$\n",
    "P(C_{i}|x) = \\frac{P(x|C_{i})P(C_{i})}{P(x)}\n",
    "= \\frac{P(x_{1}, x_{2}, ..., x_{d}|C_{i})P(C_{i})}{P(x)}\n",
    "=  \\frac{\\prod_{j=1}^{d}P(x_{j}|C_{i})P(C_{i})}{P(x)}\n",
    "$$\n",
    "\n",
    "Because  $x_{i}$ is binary, we can represent $P(x_{i}|C_{i})$ as $p(x_{i} = 1|C_{i})$ and $p(x_{i} = 0|C_{i}) = 1 - p(x_{i} = 1|C_{i})$.\n",
    "$$\n",
    "p(x_i \\mid C_{i}) = \n",
    "\\begin{cases} \n",
    "p(x_i = 1 \\mid C_{i}), & \\text{if } x_i = 1 \\\\\n",
    "1 - p(x_i = 1 \\mid C_{i}), & \\text{if } x_i = 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "$$\n",
    "= p(x_i = 1 \\mid C_{i})^{x_i} \\cdot (1 - p(x_i = 1 \\mid C_{i}))^{1 - x_i}\n",
    "$$\n",
    "\n",
    "The maximum a posteriori (MAP) decision rule is the following:\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_{y \\in \\overline{1, K}} P(C_{y}|X) \\\\\n",
    "= arg\\max_{y \\in \\overline{1, K}} \\frac{P(X|C_{y})P(C_{y})}{P(X)} \\\\ \n",
    "= arg\\max_{y \\in \\overline{1, K}} P(X|C_{y})P(C_{y}) \\\\\n",
    "$$\n",
    "\n",
    "Because P(X) is constant for all classes, we can ignore it in the argmax operation.\n",
    "\n",
    "\n",
    "$$     \n",
    "\\hat{y} = \\arg\\max_{y \\in \\overline{1, K}} P(x|C_{y})P(C_{y}) \\\\\n",
    "= \\arg\\max_{y \\in \\overline{1, K}} \\prod_{i=1}^{d} P(x_{i}|C_{y})P(C_{y}) \\\\\n",
    "= \\arg\\max_{y \\in \\overline{1, K}} \\sum_{i=1}^{d} \\log(P(x_{i}|C_{y})) + \\log(P(C_{y})) \\\\\n",
    "= \\arg\\max_{y \\in \\overline{1, K}} \\sum_{i=1}^{d} \\log(p(x_{i} = 1|C_{y})^{x_i} \\cdot (1 - p(x_{i} = 1|C_{y}))^{1 - x_i}) + \\log(P(C_{y})) \\\\\n",
    "= \\arg\\max_{y \\in \\overline{1, K}} \\sum_{i=1}^{d} x_i \\log(p(x_{i} = 1|C_{y})) + (1 - x_i) \\log(1 - p(x_{i} = 1|C_{y})) + \\log(P(C_{y}))\n",
    "$$\n",
    "\n",
    "This is a linear function of the input $\\mathbf{x}$.\n",
    "It can be written as:\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_{y \\in \\overline{1, K}} \\mathbf{w}_{y}^\\top \\mathbf{x} + b_{y}\n",
    "$$\n",
    "\n",
    "With $\\mathbf{w}_{y} = \\begin{bmatrix} \\log(\\frac{p(x_{1} = 1|C_{y})}{1 - p(x_{1} = 1|C_{y})}) \\\\ \\vdots \\\\ \\log(\\frac{p(x_{d} = 1|C_{y})}{1 - p(x_{d} = 1|C_{y})}) \\end{bmatrix}$ and $b_{y} = \\log(P(C_{y})) + \\sum_{i=1}^{d} \\log(1 - p(x_{i} = 1|C_{y}))$.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
