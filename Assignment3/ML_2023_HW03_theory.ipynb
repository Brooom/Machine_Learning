{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7c41381",
   "metadata": {},
   "source": [
    "# Home Assignment No. 3: Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b6957e",
   "metadata": {},
   "source": [
    "In this part of the homework you need to solve several theoretical problems related to machine learning algorithms.\n",
    "\n",
    "* Please include your name, surname and matriculation number in the beginning of the notebook file.\n",
    "\n",
    "\n",
    "* For every separate problem you can get **INTERMEDIATE scores**.\n",
    "\n",
    "\n",
    "* Your solution must be **COMPLETE**, i.e. contain all required formulas/proofs/detailed explanations.\n",
    "\n",
    "\n",
    "* You must write your solution for any problem right after the words **YOUR SOLUTION**. Attaching pictures of your handwriting is allowed, but **highly discouraged**.\n",
    "\n",
    "## $\\LaTeX$ in Jupyter\n",
    "\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "\n",
    "* to write **cases of equations** use \n",
    "```markdown\n",
    "$$ left-hand-side = \\begin{cases}\n",
    "                     right-hand-side on line 1, & \\text{condition} \\\\\n",
    "                     right-hand-side on line 2, & \\text{condition} \\\\\n",
    "                    \\end{cases} $$\n",
    "```\n",
    "\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "$$ \\begin{align}\n",
    "    left-hand-side on line 1 &= right-hand-side on line 1 \\\\\n",
    "    left-hand-side on line 2 &= right-hand-side on line 2\n",
    "   \\end{align} $$\n",
    "```\n",
    "\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb49fd8c",
   "metadata": {},
   "source": [
    "## Task 1. Ensembling Method [4 points]\n",
    "\n",
    "Suppose that we have random variables $X_i$ for $0 \\leq i \\leq n-1$ with $Var(X_i) = \\sigma_i^2$. Moreover, any $X_k$ and $X_l$ are correlated by a factor of ${\\rho}_{kl}$ for any k and l. Calculate the variance of the average of these random variables as in $Z = \\frac{1}{n}\\sum\\limits_{i=0}^{n-1}X_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b7c0a9",
   "metadata": {},
   "source": [
    "$\\color{red}{\\textbf{Your Solution: }}$\n",
    "\n",
    "-----------------\n",
    "$\n",
    "Var(Z) = Var(\\frac{1}{n}\\sum\\limits_{i=0}^{n-1}X_i) = \\frac{1}{n^2} Var(\\sum\\limits_{i=0}^{n-1}X_i) = \\frac{1}{n^2} (\\sum\\limits_{i=0}^{n-1} Var(X_i) + \\sum\\limits_{i\\neq j} Cov(X_i, X_j))\n",
    "$  \n",
    "$\n",
    "{\\rho}_{kl} = \\frac{Cov(X_k,X_l)}{\\sigma_k \\sigma_l}\n",
    "$  \n",
    "$\n",
    "Cov(X_k,X_l) = {\\rho}_{kl}\\sigma_k \\sigma_l = {\\rho}_{kl} \\sqrt{(Var(X_l))} \\sqrt{(Var(X_k))}\n",
    "$  \n",
    "$\n",
    "Var(Z) = \\frac{1}{n^2} (\\sum\\limits_{i=0}^{n-1} \\sigma_i^2 + \\sum\\limits_{i\\neq j} {\\rho}_{ij}\\sigma_i \\sigma_j)\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12288a6f",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "Minimizing the loss function is an optimization task, and **\"Gradient Boosting\" is one of many methods to perform optimization**. It uses a greedy approach and, therefore, may produce results that are not _globally_ optimal.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    & G_n(x) := \\text{the best base model from the family of the algorithms $\\mathcal{A}$} \\\\\n",
    "    & \\alpha_n(x) := \\text{scale or weight of the new model} \\\\\n",
    "    & f_N(x) = \\sum_{n=0}^N \\alpha_n (x) G_n(x) := \\text{the final composite model}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Gradient Boosting Algorithm\n",
    "\n",
    "Consider a loss function $\\mathcal{L}(y, \\hat{y})$ for a target $y$ and a prediction $\\hat{y}$, and let\n",
    "$(x_i, y_i)_{i = 1}^m$ be our train dataset for the regression task. \n",
    "\n",
    "\n",
    "1. Initialize $f_0(x) = z$ with the _flat constant prediction_\n",
    "\n",
    "$$z = \\arg\\min\\limits_{\\hat{y} \\in \\mathbb{R}} \\sum\\limits_{i = 1}^m \\mathcal{L}(y_i, \\hat{y})$$\n",
    "\n",
    "2. For $n$ from `1` to `n_boost_steps` do:\n",
    "    * Solve the current subporblem $L_n(G_n, \\alpha_n) \\to \\min\\limits_{G_{n}, \\alpha_n}$, where\n",
    "    \n",
    "    $$ L_n(G, \\alpha) = \\sum\\limits_{i = 1}^m \\mathcal{L}\\bigl(y_i, f_{n - 1}(x_i) + \\alpha G(x_i)\\bigr) $$\n",
    "    \n",
    "    with the following method:\n",
    "    $$\n",
    "    \\begin{align}\n",
    "      & g_i = \\left. -\\frac{\\partial \\mathcal{L}(y_i, \\hat{y})}{\\partial \\hat{y}} \\right|_{\\hat{y} = G_{n - 1}(x_i)}\n",
    "          \\\\\n",
    "      & G_n(x) = \\arg\\min\\limits_{G \\in \\mathcal{A}}\\sum\\limits_{i = 1}^l \\bigl(G(x_i) - g_i\\bigr)^2\n",
    "          \\\\\n",
    "      & \\alpha_n = \\arg\\min\\limits_\\alpha L_n(G_n, \\alpha)\n",
    "          \\\\\n",
    "      & f_n(x) = f_{n - 1}(x) + \\alpha_n G_n(x)\n",
    "    \\end{align}\n",
    "    $$\n",
    "    \n",
    "3. return $f_N(x) = f_0(x) + \\sum\\limits_{n = 1}^N \\alpha_n G_n(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49583dc",
   "metadata": {},
   "source": [
    "## Task 2. Gradient Boosting. [6 points]\n",
    "\n",
    "Assume that we've already found the optimal $G_n(x)$ at the step $n$ and we already have $f_{n-1}(x)$ from the previous step. Derive the expression for the **optimal value** of $\\alpha_n$ for the _MSE_ loss function $\\mathcal{L}(y, \\hat{y}) = (y - \\hat{y})^2$. Furthermore, explain what would happen to $\\alpha_n$ if the $|y_i - f_{n-1}(x_i)|$ values are close to 0 or significantly greater than $G_n(x_i)$ where $x_i$'s are data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54429a3",
   "metadata": {},
   "source": [
    "$\\color{red}{\\textbf{Your solution: }}$\n",
    "\n",
    "-----------------\n",
    "$\n",
    "\\frac{\\partial L_n}{\\partial \\alpha_n} L_n(G, \\alpha_n) = \\frac{\\partial L_n}{\\partial \\alpha_n} \\sum\\limits_{i = 1}^m (y_i - f_{n - 1}(x_i) - \\alpha_n G(x_i)\\bigr)^2 = \\sum\\limits_{i = 1}^m 2(y_i - f_{n - 1}(x_i) - \\alpha_n G(x_i)\\bigr)G(x_i)=\\sum\\limits_{i = 1}^m (y_i G(x_i) - f_{n - 1}(x_i) G(x_i) - \\alpha_n G(x_i)G(x_i))=0\n",
    "$  \n",
    "$\n",
    "\\alpha_n = \\frac{\\sum\\limits_{i = 1}^m y_i G(x_i) - f_{n - 1}(x_i) G(x_i)}{\\sum\\limits_{i = 1}^m G(x_i)G(x_i)}\n",
    "$  \n",
    "\n",
    "#### Case $|y_i - f_{n-1}(x_i)|$ is small:\n",
    "The model already represents the model well. Therefore it only has to make slight changes. This is why $\\alpha_n$ is close to zero.\n",
    "\n",
    "#### Case $|y_i - f_{n-1}(x_i)|$ is big:\n",
    "The model does not represent the model well. Therefore it has to make significant changes. This is why $\\alpha_n$ is large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a166f8f2",
   "metadata": {},
   "source": [
    "## Task 3. AdaBoost Weight Updates [3 points]\n",
    "\n",
    "$\\alpha_m$ parameter in AdaBoost update goes to infinity when ${err}_m$ goes to 0. What are the implications of this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9d6e25",
   "metadata": {},
   "source": [
    "$\\color{red}{\\textbf{Your solution: }}$\n",
    "\n",
    "-----------------\n",
    "This means that that all the data points are classified correctly. Because we multiply $\\alpha_m$ with 1 if $y_i$ is classified wrong and with 0 if classified right, and all the data points are classified correctly, the weight do not change. This means that the model is perfect in the sense, that it perfectly predicts the training data. This means, that the model can't learn anything from the data anymore. It is also likely, that the model is now overfitted to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361327f0",
   "metadata": {},
   "source": [
    "## Task 4. Expectation Maximization (EM) [4 points]\n",
    "\n",
    "Prove that the following equation hold\n",
    "\n",
    "$max(\\sum\\limits_{i}logp(x^{(i)}; \\theta)) \\geq max(\\sum\\limits_{i}\\sum\\limits_{z^{(i)}} Q_i(z^{i})log\\frac{p(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71bb57b",
   "metadata": {},
   "source": [
    "$\\color{red}{\\textbf{Your solution: }}$\n",
    "\n",
    "-----------------\n",
    "$\n",
    "\\frac{\\partial^2}{\\partial x^2} \\log(x) = -\\frac{1}{x^2} < 0\n",
    "$\n",
    "Therefore the log function is concave.\n",
    "\n",
    "$\n",
    "\\sum_i \\log p(x^{(i)}; \\theta) = \\sum_i \\log \\sum_{z^{(i)}} p(x^{(i)}, z^{(i)}; \\theta) = \\sum_i \\log \\sum_{z^{(i)}} Q_i(z^{(i)}) \\frac{p(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})}\n",
    "$  \n",
    "\n",
    "$Q_i(z)$ is some probability distribution over z.   \n",
    "$\n",
    "\\sum_z Q_i(z) = 1\n",
    "$\n",
    "and \n",
    "$\n",
    "Q_i(z) > 0\n",
    "$\n",
    "Therefore we can use Jensen's inequality.  \n",
    "$\n",
    "\\sum_i \\log \\sum_{z^{(i)}} Q_i(z^{(i)}) \\frac{p(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})} \\geq \\sum\\limits_{i}\\sum\\limits_{z^{(i)}} Q_i(z^{i})log\\frac{p(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})}\n",
    "$  \n",
    "Because this is true for all $Q_i(z)$, it is also true for the maximum. Therefore the equation holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b32c61b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
